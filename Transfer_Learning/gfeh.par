# comment lines start with `#`
#
# PARAMETER FILE FOR THE METALLICITY PREDICTION MODEL FOR THE Gaia G WAVEBAND
#
# simply comment/uncomment the relevant lines for retraining / deployment

--seed    42

# EXECUTION MODE:
# ---------------

--train
#--predict
#--refit        # leave it commented if training a k-fold ensemble
--cross_validate
--nn_type    rnn
--cpu           # uncomment it if you do not have GPU

# ----------------------------------------------------------------------------------------------------------------------
# I/O PARAMETERS:
# ---------------


--verbose    0


# Full path of the root directory (all other directory and file names will be relative to this).
--rootdir    ./

--outdir     Crestani_T/results

--input_model_dir     results_g/best_model_g

########## Relative path of the directory containing the input light curves.
############################################################################
##########    TRAIN (G):
#--lcdir    lc_dev_g
##########    DEPLOY on full Gaia DR2 sample:
--lcdir    Crestani_T/DR3_LC_g

########## The file containing the metadata of the stars.
#########################################################
##########    TRAIN:
#--input_file  o4rrab_gaiaDR2_bp_rp_i_g_param.dat
##########    DEPLOY on full Gaia DR2 sample:
--input_file  Crestani_T/meta_train.dat

# Output file with the predictions:
--target_output_file    Crestani_T/target_g.out

########## Input Gaia wavebands.
################################
#--wavebands    g bp rp
--wavebands    g


########## Suffices of the light-curve files (must match the --wavebands arguments)
###################################################################################
--lcfile_suffices    .dat
# --lcfile_suffices    _bin.dat _bin.dat _bin.dat

# Filename suffices for the predictions on the training / validation / test data:
--predict_train_output    Crestani_T/predict_train_gfeh
--predict_val_output      Crestani_T/predict_val_gfeh
--predict_test_output     Crestani_T/predict_test_gfeh

--plot_input_data
#--n_aug

--save_model
#--save_checkpoints         # useful if you expect the training to be interrupted, and want to resume it later

# File for saving/loading the model architecture and weights:
--model_file_prefix    model
--weights_file_prefix    weights

# File for saving/loading the standard scaling coefficients of the metadata:
--metascaler_file    gfeh_scaler

--log_training

--plot_prediction

# ----------------------------------------------------------------------------------------------------------------------
# DATA PARAMETERS:      [don't change these if you deploy the trained model]
# ----------------
#--nbins

--max_phase    1.0

##########    TRAIN (G):
#--columns   id period snr totamp phcov Nep meanmag FeH FeH_e
##########    DEPLOY on full Gaia DR2 sample:
--columns    id period snr totamp phcov Nep meanmag FeH FeH_e

##########    TRAIN (G):
--features    id period totamp FeH FeH_e
##########    DEPLOY on full Gaia DR2 sample:
#--features    id period totamp

##########    TRAIN (G):
--subset    period>0.28 and period<0.98 and totamp<1.4 and phcov>0.85 and Nep>20 and snr>30 and FeH>-2.7 and FeH<0.0 and FeH_e<0.3

##########    DEPLOY on full Gaia DR2 sample:
#--subset    period>0.28 and period<0.98 and totamp<1.4 and phcov>0.85 and Nep>20 and snr>30

# uncomment this only if using a model with second input layer for metadata:
#--meta_input period
#--meta_input period totamp_g totamp_bp totamp_rp

--explicit_test_frac    0.3
--weighing_by_density    0.5

# TRAINING PARAMETERS
# -------------------
--eval_metric    r2
--k_fold    5
--ensemble
--split_frac 0.4
--n_repeats 1
#--pick_fold    2
--n_epochs    5000  # 100000
--auto_stop    early
--min_delta    1e-5    # 1e-6
--patience    1000
--batch_size_per_replica    256
--lr    0.01
--n_zoom    200
--n_update    100
--optimize_lr
--decay    0.00005

# MODEL PARAMETERS:
# -----------------

--model    bilstm2p

--hpars 16 16 l1 5e-6 5e-6 0 0 0.1 0.1           # best model settings

#--hpars    16  16  l1  5e-6  5e-6  5e-6  5e-6   0   0
#--hpars    16  16  l1  3e-6  3e-6  5e-6  5e-6   0   0
#--hpars    16  16  l1  1e-6  1e-6  5e-6  5e-6   0   0

#--hpars    16  16  l1  5e-6  5e-6  3e-6  3e-6   0   0
#--hpars    16  16  l1  3e-6  3e-6  3e-6  3e-6   0   0
#--hpars    16  16  l1  1e-6  1e-6  3e-6  3e-6   0   0

#--hpars    16  16  l1  5e-6  5e-6  1e-6  1e-6   0   0
#--hpars    16  16  l1  3e-6  3e-6  1e-6  1e-6   0   0
#--hpars    16  16  l1  1e-6  1e-6  1e-6  1e-6   0   0

# 32 units:
#--hpars    32  32  l1  5e-6  5e-6  0  0  0.5  0.5
#--hpars    32  32  l1  3e-6  3e-6  0  0  0.5  0.5
#--hpars    32  32  l1  1e-6  1e-6  0  0  0.5  0.5

#--hpars    32  32  l1  5e-6  5e-6  0  0  0.3  0.3
#--hpars    32  32  l1  3e-6  3e-6  0  0  0.3  0.3
#--hpars    32  32  l1  1e-6  1e-6  0  0  0.3  0.3

#--hpars    32  32  l1  5e-6  5e-6  0  0  0.1  0.1
#--hpars    32  32  l1  3e-6  3e-6  0  0  0.1  0.1
#--hpars    32  32  l1  1e-6  1e-6  0  0  0.1  0.1
